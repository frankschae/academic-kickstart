---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "GSoC 2020: High weak order SDE solvers and their utility in neural SDEs"
subtitle: "Community bonding period"
summary: ""
authors: []
tags: [GSoC 2020, High weak order solver, SRK methods, Adjoint sensitivity methods]
categories: []
date: 2020-05-30T15:10:33+02:00
lastmod: 2020-05-30T15:10:33+02:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

First and foremost, I would like to thank Chris Rackauckas, Moritz Schauer, and Yingbo Ma for their willingness to supervise me in this Google Summer of Code project.
Although we are still at the very beginning of the project, we already had plenty of very inspiring discussion. I will spend the following months implementing both  **new high weak order solvers** as well as **adjoint sensitivity methods** for stochastic differential equations (SDEs).
Ultimately, these tools will allow researchers to simulate (or even control) the dynamics as generated by SDEs. Such set-ups are found in many fields ranging from the simulation of (bio-)chemical processes over financial modeling to quantum mechanics.


## High Weak Order Solvers

Currently, the [StochasticDiffEq](https://github.com/SciML/StochasticDiffEq.jl) package contains state-of-the-art solvers for the strong approximation of SDEs, i.e., solvers that allow one to reconstruct correctly the numerical solution of a SDE in a pathwise sense.
However, in many situations methods for the weak approximation, which only aim for computing an estimation for the expected value of the solution, are sufficient.  The less restrictive formulation of weak methods has the advantage that they are computationally cheaper than strong methods.


### Second order Runge-Kutta methods for Ito SDEs

In the beginning of the community bonding period I finished the implementations of the `DRI1()`[^1] and `RI1()`[^2] methods. Both are representing second order Runge-Kutta schemes and were introduced by Rößler. Interestingly, these methods are designed to scale well with the number of Wiener processes `m`. Specifically, only `2m-1` random variables have to be drawn and, additionally, the number of function evaluations for the drift and the diffusion terms is independent of `m`.


As an example, we can check the second order convergence property on a multi-dimensional SDE with non-commuting noise[^1]:

$$
\scriptstyle d \begin{pmatrix} X^1 \\\\  X^2 \end{pmatrix} = \begin{pmatrix} \frac{-273}{512} &  \phantom{X^2}0 \\\\  -\frac{1}{160} \phantom{X^2}  & -\frac{785}{512}+\frac{\sqrt{2}}{8} \end{pmatrix}  \begin{pmatrix} X^1 \\\\  X^2 \end{pmatrix} dt + \begin{pmatrix} \frac{1}{4} X^1 &  \frac{1}{16} X^1 \\\\  \frac{1-2\sqrt{2}}{4} X^2 & \frac{1}{10}X^1  +\frac{1}{16} X^2 \end{pmatrix} d \begin{pmatrix} W^1 \\\\  W^2 \end{pmatrix}    
$$




with initial value $$ X(t=0)=  \begin{pmatrix} 1 \\\\ 1\end{pmatrix}.$$

If we choose a function $f(x)=(x^1)^2$, we obtain

$$ \rm{E}\left[ f(X(t)) \right] =  \exp(-t)$$

for the expectation value of the solution.

A solution of this problem for a fixed `dt` can now be obtained by the following lines of code:

```julia
using StochasticDiffEq
numtraj = 1e7
u₀ = [1.0,1.0]
function f!(du,u,p,t)
  du[1] = -273//512*u[1]
  du[2] = -1//160*u[1]-(-785//512+sqrt(2)/8)*u[2]
end
function g!(du,u,p,t)
  du[1,1] = 1//4*u[1]
  du[1,2] = 1//16*u[1]
  du[2,1] = (1-2*sqrt(2))/4*u[1]
  du[2,2] = 1//10*u[1]+1//16*u[2]
end
dt = 1//8
tspan = (0.0,10.0)
prob = SDEProblem(f!,g!,u₀,tspan,noise_rate_prototype=zeros(2,2))

h(z) = z^2

ensemble_prob = EnsembleProblem(prob;
        output_func = (sol,i) -> (h(sol[end][1]),false)
        )
solve(ensemble_prob, DRI1();
        dt=dt,
        save_start=false,
        save_everystep=false,
        weak_timeseries_errors=false,
        weak_dense_errors=false,
        trajectories=numtraj)

```

Finally, the convergence plot displays nicely the second order convergence (slope $\approx 2.2$)

{{< figure library="true" src="DRI1.pdf" title="" lightbox="true" >}}


In the next couple of weeks, my focus will be on

* adding other high weak order solvers,
* implementing adaptive time stepping.

More of our near-term goals are collected in this [issue](https://github.com/SciML/StochasticDiffEq.jl/issues/182).


## Adjoint sensitivity methods for SDEs

The adjoint sensitivity method is well known to compute gradients of solutions to ordinary differential equations (ODEs). Recently, this method was generalized to SDEs[^3]. Importantly, this new method has different complexities in terms of memory consumption or computation time as compared with forward- or reverse-mode automatic differentiation (AD) approaches. While forward mode AD is memory efficient, it scales poorly in time with increasing number of parameters. On the contrary, reverse-mode AD, i.e., a direct backpropagation through the solver, has a huge memory footprint.

I started to implement the algorithm[^3] (c.f., Fig. 2) in a slightly modified way: To compute the gradients, we reverse the SDE such that we run the time evolution from $t_1$ to $t_0$ (instead of a evolution from $-t_1$ to $-t_0$). Here, $t_0$  represents the start time and $t_1$  represents the end time of the time evolution. This modification allows us to reuse/generalize many functions that were implemented for ODE adjoints earlier.

### Reverse SDE time evolution

The reversion of a SDE is less trivial than the reversion of an ODE. However, for SDEs written in the Stratonovich sense, it turns out that reversion can be achieved by negative signs in front of the drift and diffusion terms.
After some fixes for the available noise processes, we are now able to reverse a stochastic time evolution either by using a very general `NoiseWrapper` that interpolates in a distributionally-exact manner based on Brownian bridges, or by using `NoiseGrid` which linearly interpolates between values of the noise on a given grid.

As an example, the code below computes the forward evolution of a SDE

$$ dX  =  \alpha X dt + \beta X dW$$

with $\alpha=1.01$, $\beta=0.87$, $x(0)=1/2$, in the time span (0,1), and, subsequently, also the reverse time evolution (1,0) with initial value $x(t=1)$.

```julia
  using StochasticDiffEq, DiffEqNoiseProcess

  α=1.01
  β=0.87

  dt = 1e-3
  tspan = (0.0,1.0)
  u₀=1/2

  tarray =  collect(tspan[1]:dt:tspan[2])

  f!(du,u,p,t) = du .= α*u
  g!(du,u,p,t) = du .= β*u


  prob = SDEProblem(f!,g!,[u₀],tspan)
  sol =solve(prob,EulerHeun(),dt=dt,save_noise=true, adaptive=false)

  _sol = deepcopy(sol) # to make sure the plot is correct
  W1 = NoiseGrid(reverse!(_sol.t),reverse!(_sol.W.W))
  prob1 = SDEProblem(f!,g!,sol[end],reverse(tspan),noise=W1)
  sol1 = solve(prob1,EulerHeun(),dt=dt)
```

{{< figure library="true" src="fb.pdf" title="" lightbox="true" >}}

### Gradients of diagonal SDEs

I have already started to implement the stochastic adjoint sensitivity method for SDEs possessing diagonal noise. Currently, only non-mutating SDE functions are supported but I am optimistic that soon also the inplace formulation works.

Let us consider again the linear SDE with multiplicative noise from above (with the same parameters). This SDE represents one of the few exact solvable cases. In the Stratonovich sense, the solution is given as

$$ X(t) =  X(0) \exp(\alpha t + \beta W(t)).$$

We might be interested in optimizing the parameters $\alpha$ and $\beta$ to minimize a certain loss function acting on the solution $X(t)$. For such an optimization task, a useful search direction is indicated by the gradient of the loss function with respect to the parameters. The latter however requires the differentiation through the SDE solver -- if no analytical solution of the SDE is available.

As a prototypical scenario, let us consider a mean squared error loss

$$
  L(X(t)) = \sum_i |X(t_i)|^2,
$$

acting on the solution $X(t)$ for some fixed time points $t_i$. Then, the analytical forms for the gradients here read

$$
\begin{aligned}
  \frac{d L}{d \alpha} &= 2 \sum_i t_i |X(t_i)|^2 \\\\
  \frac{d L}{d \beta}  &= 2 \sum_i W(t_i) |X(t_i)|^2
\end{aligned}
$$

for $\alpha$ and $\beta$, respectively. We can confirm that this agrees with the gradients as obtained by the stochastic adjoint sensitivity method

```julia
using Test, LinearAlgebra
using DiffEqSensitivity, StochasticDiffEq
using Random

seed = 100
Random.seed!(seed)

u₀ = [0.5]
tstart = 0.0
tend = 0.1
dt = 0.005
trange = (tstart, tend)
t = tstart:dt:tend
tarray = collect(t)

function g(u,p,t)
  sum(u.^2.0)
end

function dg!(out,u,p,t,i)
  (out.=-2.0*u)
end

p2 = [1.01,0.87]

f(u,p,t) = p[1]*u
σ(u,p,t) = p[2]*u


Random.seed!(seed)
prob = SDEProblem(f,σ,u₀,trange,p2)
sol = solve(prob,RKMil(interpretation=:Stratonovich),dt=tend/1e7,adaptive=false,save_noise=true)
res_u0, res_p = adjoint_sensitivities(sol,EulerHeun(),dg!,t,dt=tend/1e7,sensealg=BacksolveAdjoint())


noise = vec((@. sol.W(tarray)))
Wextracted = [W[1][1] for W in noise]
resp1 = 2*sum(@. tarray*u₀^2*exp(2*(p2[1])*tarray+2*p2[2]*Wextracted))
resp2 = 2*sum(@. Wextracted*u₀^2*exp(2*(p2[1])*tarray+2*p2[2]*Wextracted))
resp = [resp1, resp2]

@test isapprox(res_p', resp, rtol = 1e-6)
# True 
```

With respect to the adjoint sensitivity methods, we are looking forward

* to finish the current backsolve adjoint version,
* to allow for computing the gradients of non-commuting SDEs,
* to implement also an interpolation adjoint version


in the upcoming weeks. For more information, the interested reader might take a look at the open [issues](https://github.com/SciML/DiffEqSensitivity.jl/issues) in the DiffEqSensitivity package.


[^1]: Kristian Debrabant, Andreas Rößler, Applied Numerical Mathematics **59**, 582–594 (2009).
[^2]: Andreas Rößler, Journal on Numerical Analysis **47**, 1713–1738 (2009).
[^3]: Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud, arXiv preprint arXiv:2001.01328 (2020).
